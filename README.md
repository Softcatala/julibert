# Introduction

BERT models were introduced by Google in 2018 and achieved state-of-the-art performance on a number of natural language understanding tasks:

* GLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks)
* SQuAD (Stanford Question Answering Dataset) v1.1 and v2.0.
* SWAG (Situations With Adversarial Generations)

Google [has published](https://github.com/google-research/bert) two sets of models:

* Single language models for English and Chineses
* [Multilngual models](https://github.com/google-research/bert/blob/master/multilingual.md) where a single model covers 104 languages

It has been prove the multilingual models perform poorly comparared to single language models. Serveral linguistics communities like French, Finish or Spainish have been working on creating the language specific models that outperform Google multilingual models.

# Goals of the project

This project has two goals

## Create a Catalan BERT alike model for Catalan language 

## Evalute its use as part of our grammar correction system

# Published model















